{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS 382N: ADVANCED PREDICTIVE MODELING - MSBA</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5 - BONUS - SECTION-A (3.30pm to 5pm class)</p>\n",
    "## <p style=\"text-align: center;\">Points: 95, Bonus: 40 </p>\n",
    "## <p style=\"text-align: center;\">Due: November 30, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Your partner needs to be from the same section. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTEID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TA know. \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "\n",
    "### Name(s)\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 : Bayesian Belief Networks (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question 1](https://i.imgur.com/3ayVbFi.jpeg) \n",
    "\n",
    "All nodes are binary and can take 0/1 values\n",
    "\n",
    "The probabilities are given below:\n",
    "\n",
    "P(Bark = 1) = 0.05   \n",
    "P(Rain = 1) = 0.01\n",
    "\n",
    "\n",
    "P(Scared = 1 | Bark = 0, Rain = 0) = 0.001  \n",
    "P(Scared = 1 | Bark = 0, Rain = 1) = 0.1  \n",
    "P(Scared = 1 | Bark = 1, Rain = 0) = 0.8  \n",
    "P(Scared = 1 | Bark = 1, Rain = 1) = 0.9  \n",
    "\n",
    "P(Hides = 1 | Scared = 1) = 0.95  \n",
    "P(Hides = 1 | Scared = 0) = 0.05  \n",
    "\n",
    "For the given Bayesian network, Compute the following probabilities :  \n",
    "\n",
    "\n",
    "**(a) (4 pts)** Find the probability that cat hides = 1.  \n",
    "**(b) (4 pts)** Given that cat got scared (Scared = 1), what is the probability that it rained (Rain = 1)?  \n",
    "**(c) (7 pts)** Given that cat got scared (Scared = 1) and the dog barked (Bark = 1), what is the probability that it rained (Rain = 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - **Random Forest (30 pts)**  \n",
    "\n",
    "The goal of this problem is to explore the effect of feature selection using  the following dataset (same as in HW 4) https://www.kaggle.com/wendykan/lending-club-loan-data\n",
    "\n",
    "Since the dataset is really huge, we will use only a certain set of features and samples to build our model. We will also use only two classes instead of all.\n",
    "\n",
    "The modified dataset with reduced number of samples has been prepared for your use. The file can be downloaded using the code below. You can also find it [here](https://drive.google.com/file/d/1Gv_N1rHLqDizxUck6l06BfniR30Pw0Zs/view?usp=sharing) in case you have a different environment and this code does not run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ggID='1Gv_N1rHLqDizxUck6l06BfniR30Pw0Zs'  \n",
    "ggURL='https://drive.google.com/uc?export=download'  \n",
    "filename=\"$(curl -sc /tmp/gcokie \"${ggURL}&id=${ggID}\" | grep -o '=\"uc-name.*</span>' | sed 's/.*\">//;s/<.a> .*//')\"  \n",
    "getcode=\"$(awk '/_warning_/ {print $NF}' /tmp/gcokie)\"  \n",
    "\n",
    "if [ -e \"${filename}\" ]; then\n",
    "    echo 'File already exists'\n",
    "else\n",
    "    curl -Lb /tmp/gcokie \"${ggURL}&confirm=${getcode}&id=${ggID}\" -o \"${filename}\"  \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('loan.csv')\n",
    "df = dataset.fillna(0)\n",
    "\n",
    "def LoanResult(status):\n",
    "    if (status == 'Fully Paid') or (status == 'Current'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['loan_status'] = df['loan_status'].apply(LoanResult)\n",
    "\n",
    "df = df[['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n",
    "             'emp_length', 'home_ownership','annual_inc', 'verification_status', 'loan_status',\n",
    "             'purpose','addr_state', 'dti','open_acc', 'pub_rec', 'revol_bal', 'revol_util', \n",
    "             'initial_list_status', 'recoveries','collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
    "             'application_type', 'tot_coll_amt', 'tot_cur_bal', 'avg_cur_bal', 'chargeoff_within_12_mths',\n",
    "             'pub_rec_bankruptcies', 'tax_liens', 'debt_settlement_flag']]\n",
    "df_cat = df.select_dtypes(exclude=['int64', 'float64'])\n",
    "df = pd.get_dummies(df, df_cat.columns.values)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the `loan_status` column as the target column.  \n",
    "\n",
    "\n",
    "**Part 1: (3 pts)** Split the dataset into train and test set with 25% data in test set and random_state = 42. Perform [Min-Max Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) on the dataset. Print the total number of features. \n",
    "\n",
    "**Part 2: (5 pts)** Use the dataset to create a `RandomForestClassifier(n_estimators=5, random_state=42)` model and print the [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to show the precision, recall and F1 score based on the test set.  \n",
    "\n",
    "**Part 3: (6 pts)** Use [$\\chi^2$ test](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) to select the top 100, 30 and 10 features using [SelectKBest](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) module. Train a random forest model and print the [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) for each of the trained models.  \n",
    "\n",
    "**Note:** $\\chi^2$ test requires non-negative values only for all features. Since we have performed Min-Max scaling previously (where the default behavior is to scale features between 0 and 1, there should not be any problem performing this test.)   \n",
    "\n",
    "**Part 4: (6 pts)** [Plot the ROC curves](https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/21_plot_roc_curve.ipynb) for all of the 4 models.  \n",
    "\n",
    "**Part 5: (8 pts)** Plot the feature importance for each of the 4 random forest models that you have trained to show the top 5 features. ([Get feature names after feature selection](https://stackoverflow.com/a/43765224), [Plot feature importance using Pandas and matplotlib](https://stackoverflow.com/a/51520906))  \n",
    "\n",
    "**Part 6: (2 pts)** What do you observe from the ROC curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 : Logistic Regression with Regularization and Decision Tree (25 pts)\n",
    "\n",
    "The goal of this problem is to explore the effect of regularization on logistic regression for binary classification, we will be using the diabetes dataset. \n",
    "This dataset is used to predict if a person is having \n",
    "diabetes based on feature variables including blood pressure, bmi, age etc. The target variable is stored in \"outcome\" column.\n",
    "\n",
    "* Load the \"diabetes.csv\" and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. \n",
    "\n",
    "\n",
    "* We need to use [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on the original data, we use StandardScaler to center each feature. Also remember that when we have training and testing data, we fit preprocessing parameters on training data and apply them to all testing data. You should scale only the features (independent variables), not the target variable y. \n",
    "\n",
    "   Note: X should have 8 features.\n",
    "\n",
    "\n",
    "**Part 1: (6 pts)** Fit a [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model with penalty $l2$\n",
    "(Ridge Regularization) for the following values of regularization  C = $[0.0001,0.001,0.1,10,100]$ using the training data. Report the accuracy score on the test data averaged over 5 runs of the model for each of the C values. \n",
    "\n",
    "  Note : Smaller values of C indicate stronger regularization\n",
    "\n",
    "    \n",
    "\n",
    "**Part 2: (6 pts)** Fit a [Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) classifier on the training data, and report the accuracy score on the test data averaged over 5 runs. Briefly explain which of the models performed the best Logistic Regression + regularization or Decision Tree Classifier and why. \n",
    "\n",
    "\n",
    "* Now, We will check if the decision boundary from logistic regression is linear by using a 2D plot.\n",
    "\n",
    "\n",
    "**Part 3: (4 pts)** Select the top two most important features ([Feature importance](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)) using the training data with a Decision Tree Classifier and random_state = 10. Subset the train and test data to have only the selected features. This will be used as the training and test data for part $4$.\n",
    "\n",
    "\n",
    "**Part 4: (8 pts)** Fit a [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model with penalty = $l2$, C = 10 (Ridge Regularization) and random state = 0 for the training data from part $3$ . Plot the decision boundary of the logistic regression model with the two features, as X and Y axis. Here is an [example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html) . Plot the scatter points for the test data, on the same graph showing the two classes as two different color. \n",
    "\n",
    "  * Hint : You can use the below limits for plotting the decision boundary, where $X[:,0]$ indicates feature 1 values and $X[:,1]$ indicates feature 2 values of the train dataset. \n",
    "\n",
    "     $ x_\\min, x_\\max = X[:, 0].min() - .5, X[:, 0].max() + .5 $ \n",
    "\n",
    "     $ y_\\min, y_\\max = X[:, 1].min() - .5, X[:, 1].max() + .5 $\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset and pre-processing (**1 pt**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (**6 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (**6 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (**4 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (**8 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Comparison of different ensembles method for classification (25 pts)\n",
    "\n",
    "In this question, we will compare performance of different ensemble methods for classification problems: [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [GradientBoosting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) Classifiers.\n",
    "\n",
    "[Spam Classification Data](https://archive.ics.uci.edu/ml/datasets/Spambase) of UCI will be used (you can use the data provided: 'spam_uci.csv'). Don't worry about column names. The last column represents target label, 1 if spam and zero otherwise.\n",
    "\n",
    "* Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. \n",
    "\n",
    "\n",
    "\n",
    "**Part 1: (4 pts)** Use a Decision Tree Classifier with random_state = 10 and Logistic Regression with random_state = 10 and solver =\"lbfgs\" for the spam classification problem. Report the accuracy_score and roc_auc_score on the test data for each classifier.\n",
    "\n",
    "\n",
    "**Part 2: (8 pts)** Create an ensemble of 50 classifiers (i.e n_estimators = 50) with random_state = 10 for [bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) with base classifier as Decision Tree Classifier and Logistic Regression from part $1$ . Report accuracy_score and roc_auc_score on the test data for both the Bagging classifiers. Compare the results and breifly explain the effect of using bagging for the spam classification problem for both the Logistic Regression and Decision Tree base classifier.\n",
    "\n",
    "\n",
    "* Now we will look at the effect of other ensemble methods on this problem.\n",
    "\n",
    "\n",
    "**Part 3: (5 pts)** Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to classify whether an email is spam. Report your testing accuracy ([accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score). You will need [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) for roc_auc_score. \n",
    "\n",
    "\n",
    "**Part 4: (5 pts)** Use [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem.  Report  accuracy_score and roc_auc_score on the test data for each algorithm. \n",
    "\n",
    "  Note : For part 3 and 4 find the best values for the hyper parameters of each of the models by using GridSearchCV. \n",
    "\n",
    "\n",
    "**Part 5: (3 pts)** Briefly explain which of the three ensemble method above from Part 3&4 performed the best and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and pre-processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (**4 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (**8 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (**5 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (**5 pts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5 (**3 pts**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 5 (Bonus): Segmenting Customers for Predicting Loan Repayment (40 points)\n",
    "\n",
    "Download dataset from [this link](https://drive.google.com/file/d/1U6FDSQQSUzvw3Ygtgp8I5ea0-dL-orSE/view?usp=sharing).\n",
    "\n",
    "In this question, we will look at a technique to segment customers using SHAP (SHapley Additive exPlanations). Please read this article to learn about SHAP - https://christophm.github.io/interpretable-ml-book/shap.html.\n",
    "\n",
    "SHAP values provide an explanation for each observation in terms of feature attribution (which features were most influential in determining the predicted value for that observation).\n",
    "\n",
    "For some applications, that may be too fine a resolution, and one would rather identify subsets of observations (called segments or clusters) for which the “explanations are similar”, Moreover, it helps if subsets are also “understandable”, i.e. easy to describe in terms of the feature values. Then the subsets become “actionable”, as one can design specific interventions (e.g. targeted marketing messages) for each subset.\n",
    "\n",
    "The goal of this problem is to identify such segments, and see if the variation of SHAP values within each segment is indeed lower than the variation in the total population. We will proceed as follows:\n",
    "\n",
    "1. First, we will fit an XGBoost model to perform the classification task.\n",
    "2. We will then use the shap.TreeExplainer which uses Tree SHAP algorithms and estimate the SHAP values for each observation.\n",
    "3. Our objective is to segment customers into groups which have low variance in the SHAP values - these represent segments of similar customers in the “feature attribution based explanation” space. We will do this as follows:  \n",
    "\n",
    "    a. Cluster the SHAP values using KMeans clustering. You pick K, perhaps somewhere between 10 and 20.  \n",
    "    b. Build a decision tree with the Cluster numbers as the labels. Specify the number of leaves to be in the range of [K, 2K].  Due to the choice of the surrogate class labels provided by the clustering, the   leaves of the decision tree will be encouraged to have similar SHAP values. Moreover, since you are learning a decision tree, these leaves are also simple to describe as rules. A good solution will be indicated if  for each of these leaves,  the variance of SHAP values of observations mapped to that leaf will be lower than the  overall variance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA -  DONOT CHANGE\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import time\n",
    "import xgboost\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os, sys, re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the csv file and fill Nan/empty values as 0\n",
    "dataset = pd.read_csv('loan.csv')\n",
    "df = dataset.fillna(0)\n",
    "\n",
    "# We will be using only two classes and group them as below\n",
    "def LoanResult(status):\n",
    "    if (status == 'Fully Paid') or (status == 'Current'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['loan_status'] = df['loan_status'].apply(LoanResult)\n",
    "\n",
    "# Set of features which indicate the dimensionality of the data\n",
    "df = df[['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n",
    "             'emp_length', 'home_ownership','annual_inc', 'verification_status', 'loan_status',\n",
    "             'purpose','addr_state', 'dti','open_acc', 'pub_rec', 'revol_bal', 'revol_util', \n",
    "             'initial_list_status', 'recoveries','collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
    "             'application_type', 'tot_coll_amt', 'tot_cur_bal', 'avg_cur_bal', 'chargeoff_within_12_mths',\n",
    "             'pub_rec_bankruptcies', 'tax_liens', 'debt_settlement_flag']]\n",
    "\n",
    "#For simplicity, in this question, we select all columns that do not contain integer of float type of data. Then, one hot encoding is performed.\n",
    "df_cat = df.select_dtypes(exclude=['int64', 'float64'])\n",
    "df = pd.get_dummies(df, df_cat.columns.values)\n",
    "\n",
    "\n",
    "X = df\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "df.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df.columns.values]\n",
    "\n",
    "X = X.drop(['loan_status'], axis=1)\n",
    "Y = df['loan_status']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Fit an XGBoost model on the train data and report the ROCAUC on the test set. (5pts)\n",
    "### 5.2. Compute the SHAP values using the [TreeExplainer](https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html) and plot the [summary plot](https://shap.readthedocs.io/en/latest/generated/shap.summary_plot.html). Explain what the summary plot represents.(5 pts)\n",
    "### 5.3. Perform KMeans clustering on the SHAP values inot $k$ clusters. Then, train a decision tree (with maximum depth $d$) to predict the cluster number obtained through KMeans -  the cluster numbers as the target. Visualize the decison tree (5 pts). Plot the variance of the SHAP values of the samples in each of the leaves - show that all these varinces is less than the overall variance of the SHAP values. The main parameters to tune here are the number of cluster $k$ and the maximum depth $d$. **(30 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
